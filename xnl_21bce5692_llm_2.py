# -*- coding: utf-8 -*-
"""XNL-21BCE5692-LLM-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VTc1MzE6AWRxombMkMJPA3RtPpogvCIw

# TASK 2: ULTRA-ADVANCED LLM FINE-TUNING & OPTIMIZATION

##PHASE 1: INITIAL DESIGN & INFEASIBILITY ASSESSMENT
1.1 Assess LLM Fine-Tuning Requirements
"""

# Install Required Libraries
!pip install transformers datasets torch deepspeed accelerate optuna ray[tune] wandb tensorflow cloud-tpu-client --quiet

# Import Libraries
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# Step 1: Select Target LLM
MODEL_NAME = "EleutherAI/gpt-neo-1.3B"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# Step 2: Load Fine-Tuning Dataset
dataset = load_dataset("squad")
print(dataset)

"""1.2 Infrastructure & Cloud Planning"""

# Step 1: Multi-Cloud Infrastructure Design
# (Manual setup required on AWS, GCP, or Azure Free Tier)

# Step 2: High-Performance Compute Selection
# Use Colab's free GPU or upgrade to Colab Pro for better GPUs (e.g., T4, A100)
print("GPU Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0))

"""##PHASE 2: LLM FINE-TUNING FRAMEWORK SETUP & DISTRIBUTED TRAINING
2.1 Setup LLM Fine-Tuning Framework
"""

# Step 1: Model Checkpoints
# Use pre-trained weights from Hugging Face Hub
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# Step 2: Hyperparameter Tuning Tools
import optuna
from transformers import TrainingArguments, Trainer

"""2.2 Distributed Training & Optimization"""

# Step 1: Data Parallelism
# Use DeepSpeed for distributed training
!pip install deepspeed

# Step 2: Gradient Accumulation
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=5e-5,
    weight_decay=0.01,
    fp16=True,
    logging_steps=50,
    save_steps=200,
    save_total_limit=2,
    report_to="wandb"
)

# Step 3: Sharded Data Loading
# (Use PyTorch DataLoader for efficient data loading)
from torch.utils.data import DataLoader

train_loader = DataLoader(dataset["train"], batch_size=4, shuffle=True)

"""##PHASE 3: MODEL FINE-TUNING WITH ADVANCED OPTIMIZATION

3.1 Advanced Hyperparameter Tuning
"""

Define Hyperparameter Tuning Objective Function
def objective(trial):
    # Define hyperparameters to tune
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True)
    batch_size = trial.suggest_categorical("batch_size", [4, 8, 16])
    num_epochs = trial.suggest_int("num_epochs", 1, 5)

    # Update TrainingArguments with the suggested hyperparameters
    training_args = TrainingArguments(
        output_dir="./results",
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=2,
        num_train_epochs=num_epochs,
        learning_rate=learning_rate,
        weight_decay=0.01,
        fp16=True,
        logging_steps=50,
        save_steps=200,
        save_total_limit=2,
        report_to="wandb",
        remove_unused_columns=False
    )

    # Initialize Trainer with the updated training_args
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
    )

"""3.2 Data & Model Augmentation"""

# Step 1: Data Preprocessing & Augmentation
# (Use Hugging Face Datasets for tokenization and augmentation)
def tokenize_function(examples):
    return tokenizer(examples["context"], truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

"""
##PHASE 4: ADVANCED AI AGENTS FOR AUTOMATION AND OPTIMIZATION

4.1 AI Agent Integration for Monitoring & Automation"""

# Step 1: AI-Driven Model Monitoring
import wandb
wandb.init(project="llm-fine-tuning")

# Step 2: Optimization Automation
# (Use WandB for real-time monitoring and hyperparameter tuning)

"""4.2 AI-Powered Resource Allocation"""

#Step 1: Install Required Libraries
!pip install ray[tune] wandb nvidia-ml-py3 --quiet

#Step 2: Monitor GPU Usage
import pynvml  # NVIDIA Management Library for GPU monitoring

# Initialize NVIDIA ML
pynvml.nvmlInit()

# Function to monitor GPU usage
def monitor_gpu():
    device_count = pynvml.nvmlDeviceGetCount()
    for i in range(device_count):
        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        print(f"GPU {i}:")
        print(f"  Memory Used: {info.used / 1024 ** 2:.2f} MB")
        print(f"  Memory Free: {info.free / 1024 ** 2:.2f} MB")
        print(f"  Memory Total: {info.total / 1024 ** 2:.2f} MB")

import wandb

# Initialize Weights & Biases for monitoring
wandb.init(project="llm-fine-tuning")

# Simulate AI agent for auto-tuning
def ai_agent():
    # Monitor GPU usage
    monitor_gpu()

    # Simulate AI logic for auto-tuning
    gpu_usage = pynvml.nvmlDeviceGetMemoryInfo(pynvml.nvmlDeviceGetHandleByIndex(0)).used
    if gpu_usage > 8000:
        print("Triggering auto-scaling: Adding more GPUs")
        # Add more GPUs dynamically (simulated)
        return {"gpu": 2}
    else:
        return {"gpu": 1}

# Simulate training loop with AI agent
for epoch in range(10):
    # Train model
    print(f"Epoch {epoch + 1}")

    # Call AI agent for auto-tuning
    resources = ai_agent()
    print(f"Allocated Resources: {resources}")

    # Log metrics to Weights & Biases
    wandb.log({"epoch": epoch + 1, "gpu_usage": resources["gpu"]})

"""##PHASE 5: TESTING, VALIDATION, AND CONTINUOUS IMPROVEMENT

5.1 Robust Testing with AI Agents
"""

from transformers import Trainer
from evaluate import load

# Load evaluation metrics
bleu_metric = load("bleu")
rouge_metric = load("rouge")

# Evaluate the model
def evaluate_model(trainer, dataset):
    # Evaluate on the validation set
    eval_results = trainer.evaluate(eval_dataset=dataset)
    print("Evaluation Results (Loss):", eval_results)

    # Compute BLEU and ROUGE scores (for text generation tasks)
    predictions = trainer.predict(dataset).predictions
    references = dataset["answers"]

    # Compute BLEU score
    bleu_score = bleu_metric.compute(predictions=predictions, references=references)
    print("BLEU Score:", bleu_score)

    # Compute ROUGE score
    rouge_score = rouge_metric.compute(predictions=predictions, references=references)
    print("ROUGE Score:", rouge_score)

"""Step 2: Custom Test Suites"""

# Define custom test cases
custom_test_cases = [
    {"input": "What is the capital of France?", "expected_output": "Paris"},
    {"input": "Who wrote 'Pride and Prejudice'?", "expected_output": "Jane Austen"},
    # Add more test cases
]

# Test the model on custom cases
def test_custom_cases(model, tokenizer, test_cases):
    for case in test_cases:
        input_text = case["input"]
        expected_output = case["expected_output"]

        # Tokenize input
        inputs = tokenizer(input_text, return_tensors="pt")

        # Generate output
        outputs = model.generate(**inputs)
        predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Compare predicted and expected output
        print(f"Input: {input_text}")
        print(f"Expected Output: {expected_output}")
        print(f"Predicted Output: {predicted_output}")
        print("---")

# Run custom test cases
test_custom_cases(model, tokenizer, custom_test_cases)

"""5.2: Continuous Performance Monitoring


"""

#Step 1: Model Drift Detection
import numpy as np
from sklearn.metrics import accuracy_score

# Simulate model drift detection
def detect_drift(y_true, y_pred, threshold=0.05):
    accuracy = accuracy_score(y_true, y_pred)
    if accuracy < threshold:
        print(f"Model Drift Detected! Accuracy dropped to {accuracy:.2f}.")
    else:
        print(f"No Drift Detected. Accuracy: {accuracy:.2f}.")

# Simulate true and predicted labels
y_true = np.random.randint(0, 2, size=100)  # True labels
y_pred = np.random.randint(0, 2, size=100)  # Predicted labels

# Detect drift
detect_drift(y_true, y_pred)

"""##PHASE 6: MULTI-CLOUD DEPLOYMENT, MONITORING, AND SECURITY HARDENING

6.1 Multi-Cloud Deployment & Scaling
"""

#Step 1: Containerized Deployment
# Save the fine-tuned model and tokenizer
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

# Create a Dockerfile for the model
dockerfile_content = """
FROM python:3.9-slim
WORKDIR /app
COPY . /app
RUN pip install transformers torch
EXPOSE 5000
CMD ["python", "app.py"]
"""

# Save the Dockerfile
with open("Dockerfile", "w") as f:
    f.write(dockerfile_content)

# Create a simple Flask app for serving the model
app_content = """
from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

app = Flask(__name__)

# Load the fine-tuned model and tokenizer
model = AutoModelForCausalLM.from_pretrained("./fine_tuned_model")
tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")

@app.route("/predict", methods=["POST"])
def predict():
    data = request.json
    inputs = tokenizer(data["text"], return_tensors="pt")
    outputs = model.generate(**inputs)
    return jsonify({"response": tokenizer.decode(outputs[0], skip_special_tokens=True)})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
"""

# Save the Flask app
with open("app.py", "w") as f:
    f.write(app_content)

print("Dockerfile and Flask app created. Build and deploy the Docker container to your cloud platform.")

#Step 2: Auto-Scaling
# Example Kubernetes Deployment YAML with KEDA for auto-scaling
# Save this as `deployment.yaml`
deployment_yaml = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm
  template:
    metadata:
      labels:
        app: llm
    spec:
      containers:
      - name: llm-container
        image: your-docker-image:latest
        ports:
        - containerPort: 5000
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: llm-scaler
spec:
  scaleTargetRef:
    name: llm-deployment
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server:9090
      metricName: http_requests_total
      threshold: "100"
      query: sum(rate(http_requests_total{app="llm"}[1m]))
"""

# Save the deployment YAML
with open("deployment.yaml", "w") as f:
    f.write(deployment_yaml)

print("Kubernetes deployment YAML created. Apply it to your cluster using `kubectl apply -f deployment.yaml`.")

"""6.2: Security and Compliance

Step 1: Model Security
"""

# Example: Add a watermark to the model's outputs
def add_watermark(text):
    return text + " [Watermark: LLM-2023]"

# Test watermarking
output = model.generate(**tokenizer("What is the capital of France?", return_tensors="pt"))
watermarked_output = add_watermark(tokenizer.decode(output[0], skip_special_tokens=True))
print("Watermarked Output:", watermarked_output)

"""Step 2: Privacy Preservation"""

# Example: Simulate federated learning
from torch import nn, optim

# Define a simple model for federated learning
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)

# Simulate federated training
def federated_training(models, data):
    for model, dataset in zip(models, data):
        optimizer = optim.SGD(model.parameters(), lr=0.01)
        for x, y in dataset:
            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.mse_loss(output, y)
            loss.backward()
            optimizer.step()

# Simulate data for federated learning
models = [SimpleModel() for _ in range(3)]
data = [
    [(torch.randn(10), torch.randn(1)) for _ in range(10)],  # Client 1
    [(torch.randn(10), torch.randn(1)) for _ in range(10)],  # Client 2
    [(torch.randn(10), torch.randn(1)) for _ in range(10)],  # Client 3
]

# Run federated training
federated_training(models, data)
print("Federated training completed.")